{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/chefs-kiss/ML_J2026/blob/main/PA5_Trees_with_BreastCancer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Name:\n",
        "\n",
        "\n",
        "Who you worked with:"
      ],
      "metadata": {
        "id": "yGOblcCObZW_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Objectives\n",
        "The goals of this project are to:\n",
        "- Implement multiple tree-based models\n",
        "- Tune models to find the best combination of hyperparameters\n",
        "\n",
        "## Overview\n",
        "For this programming assignment, you will be working with the Wisconsin Breast Cancer dataset. This dataset contains various measurements of the size and shape of tumors, along with the diagnosis of the tumor as benign or malignant. Further information on this dataset can be found here: https://archive.ics.uci.edu/ml/datasets/breast+cancer+wisconsin+(diagnostic). Our goal is to train a tree-based model that can use the measurements of a tumor to diagnosis it as benign or malignant.\n",
        "\n",
        "## Schedule\n",
        "Here is the suggested schedule for working on this project:\n",
        "- Weekend: Read through project instructions, run code for Task 0.\n",
        "- Tuesday: Complete Tasks 1-2.\n",
        "- Wednesday: Complete Tasks 3.\n",
        "- Thursday: Complete Task 4.\n",
        "\n",
        "This project is due on Thursday, 3/20, by 11:59pm.\n"
      ],
      "metadata": {
        "id": "PNPIK_xyNuDK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Task 0: Data\n",
        "\n",
        "We start by loading the dataset into a pandas dataframe."
      ],
      "metadata": {
        "id": "zTaoKf55giYQ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kCUf5sZw9rrA"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "cancer_df = pd.read_csv(\"https://github.com/lynn0032/MLCamp2021/raw/main/breast_cancer.csv\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, we look at a summary of the data. Note that not every column is shown in this summary."
      ],
      "metadata": {
        "id": "CZCyiNJtlmRN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cancer_df.describe(include = \"all\")"
      ],
      "metadata": {
        "id": "5Ap6YnLA-D65"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can use shape to check the size of the data. Here, we see that we have 569 samples (for 569 tumors), and 33 attributes."
      ],
      "metadata": {
        "id": "8JEGcWWClyse"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cancer_df.shape"
      ],
      "metadata": {
        "id": "LZ5BN1mflyV0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Normally, we'd do exploratory data analysis here to understand the distribution of the dataset. For this programming assignment, however, we're going to focus on evaluation for a classifier, so we will neglect to do that here.\n",
        "\n",
        "There are two columns that we won't use for prediction, so we drop those columns."
      ],
      "metadata": {
        "id": "7mLS2Q03mEt1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cancer_df.drop([\"id\", \"Unnamed: 32\"], axis=1, inplace = True)\n",
        "\n",
        "cancer_df.describe(include=\"all\")"
      ],
      "metadata": {
        "id": "kDyW_2DC-O0F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, we separate the dataset into features and target. Remember that the diagnosis is the target, and it takes values \"M\" (for malignant) and \"B\" for benign."
      ],
      "metadata": {
        "id": "Uy4vqeTEmXf7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "features = list(cancer_df.columns)\n",
        "features.remove(\"diagnosis\")\n",
        "\n",
        "X = cancer_df[features].values\n",
        "y = cancer_df[\"diagnosis\"].values"
      ],
      "metadata": {
        "id": "DF3ny52J-o6s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task 1: Evaluation for multiple random testing-training splits\n",
        "\n",
        "First, we import the functions that we'll use to train and evaluate models."
      ],
      "metadata": {
        "id": "QCeM5JuemkQ7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score"
      ],
      "metadata": {
        "id": "qjkB4u-Sw00k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "To see how a decision tree (with default parameter values) performs on this data, we will generate random testing training splits in repeated trials, and average the performance. You will want to reference the workbook from Friday to figure out where to put the sklearn methods.\n",
        "\n",
        "##üíªQ1: Complete the code\n",
        "\n",
        "*   We will run 10 trials, generating 10 different training-testing splits.\n",
        "*   For each trial, we generate a new training-testing split, train a model on the training set, and evaluate it on the testing set.\n",
        "*   Each training-testing split should use 75% of the data for training (then how much is used for testing?), and for a random state, use the value of the variable `iter` (giving the trial number).\n",
        "*   Create a decision tree classifier with random state 189, and train it on the training data.\n",
        "*   Make predictions for the testing set\n",
        "*   Compute the accuracy, precision, and recall for your model on the testing data, and append these to the lists storing these results. Use \"M\" as the positive class.\n",
        "*   For precision and recall, you will need to use an optional parameter, `pos_label`, to specify the label for the positive class (\"M\")."
      ],
      "metadata": {
        "id": "jNPfXCugm-IX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "num_trials =   # your code here\n",
        "\n",
        "accuracy = []\n",
        "precision = []\n",
        "recall = []\n",
        "\n",
        "for iter in range(num_trials):\n",
        "  # perform a random split\n",
        "  # your code here\n",
        "\n",
        "\n",
        "  # train classifier on the training data\n",
        "  # your code here\n",
        "\n",
        "\n",
        "  # make predictions on the testing data\n",
        "  # your code here\n",
        "\n",
        "\n",
        "  # compute and store the performance metrics\n",
        "  # your code here\n",
        "\n",
        "\n",
        "# report the performance\n",
        "print(\"Average Accuracy:\", sum(accuracy)/len(accuracy))\n",
        "print(\"Average Precision:\", sum(precision)/len(precision))\n",
        "print(\"Average recall:\", sum(recall)/len(recall))\n",
        "print(\"Accuracy:\", accuracy)\n",
        "print(\"Precision\", precision)\n",
        "print(\"Recall\", recall)"
      ],
      "metadata": {
        "id": "_RRyS7HmAn4I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# TEST CELL - DO NOT CHANGE - RUN THIS CELL TO CHECK YOUR WORK\n",
        "# Note: passing this cell doesn't guarantee your code is correct or that you will get full credit,\n",
        "# but should be used to help you check your work\n",
        "assert(accuracy[3:6] == [0.9300699300699301, 0.8951048951048951, 0.958041958041958])\n",
        "assert(precision[7:10] == [0.9285714285714286, 0.8928571428571429, 0.9411764705882353])\n",
        "assert(recall[-3:] == [0.8666666666666667, 0.8620689655172413, 0.9056603773584906])\n",
        "print(\"Tests passed, but be sure to test your own code as well!\")"
      ],
      "metadata": {
        "id": "Puf0-qPnO92R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##‚úè Q2: Interpreting Your Results\n",
        "\n",
        "In the cell below, for precision and recall, write one sentence each describing what these mean in the context of the models and dataset.\n",
        "\n",
        "As an example, here is such a sentence for accuracy: Of all the tumors in the dataset, our models classified an average of 91.8% of tumors correctly as malignant or benign."
      ],
      "metadata": {
        "id": "92Bje5lZo8sr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "your answers here"
      ],
      "metadata": {
        "id": "QE457t5KpbF1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Task 2: Evaluation\n",
        "\n",
        "For this task, you will complete the definition of an evaluation function that can be used to evaluate the performance of a decision tree with specified values for the parameters `max_depth` and `min_samples_leaf`. We will use stratified $k$-fold cross-validation, to help ensure that our results are reliable.\n"
      ],
      "metadata": {
        "id": "QWf1OvaVqEYZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##üíª Q3: Complete the code\n",
        "\n",
        "*   Use stratified $k$-fold validation, with $k=5$. Remember to shuffle the data, and use a random state of 7.\n",
        "*   For each split, you will train a decision tree classifier with the the values of `max_depth` and `min_samples_leaf` given by the function parameters. Use random state 7.\n",
        "*   For each model trained, you will compute and store the accuracy on the testing set and the accuracy on the training set. The function returns the average testing accuracy and the average training accuracy."
      ],
      "metadata": {
        "id": "FzI6Al1dRJkc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import StratifiedKFold\n",
        "\n",
        "def evaluate(max_depth_value, min_samples_leaf_value):\n",
        "  # Use stratified 5-fold evaluation to generate splits\n",
        "  # your code here\n",
        "\n",
        "\n",
        "  test_accuracy = []\n",
        "  train_accuracy = []\n",
        "\n",
        "  # for each split\n",
        "  for train_index, test_index in skf.split(X, y):\n",
        "    # Get training and testing sets\n",
        "    X_train, X_test = # your code here\n",
        "    y_train, y_test = # your code here\n",
        "\n",
        "    # Train a decision tree classifier on the training set\n",
        "    # your code here\n",
        "\n",
        "\n",
        "    # Make predictions on the testing set\n",
        "    y_test_pred = # your code here\n",
        "\n",
        "    # Make predictions on the training set\n",
        "    y_train_pred = # your code here\n",
        "\n",
        "    # Evaluate the performance\n",
        "    test_accuracy.append(accuracy_score(y_test, y_test_pred))\n",
        "    train_accuracy.append(accuracy_score(y_train, y_train_pred))\n",
        "\n",
        "  return sum(test_accuracy)/len(test_accuracy), sum(train_accuracy)/len(train_accuracy)"
      ],
      "metadata": {
        "id": "JRzh-i-FxCDR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# TEST CELL - DO NOT CHANGE - RUN THIS CELL TO CHECK YOUR WORK\n",
        "# Note: passing this cell doesn't guarantee your code is correct or that you will get full credit,\n",
        "# but should be used to help you check your work\n",
        "assert(evaluate(10,2) == (0.9227138643067848, 0.9890138808559861))\n",
        "assert(evaluate(4,10) == (0.9139108834031984, 0.9617765567765568))\n",
        "assert(evaluate(4,1) == (0.9262381617761217, 0.9859417775207249))\n",
        "assert(evaluate(None,20) == (0.9104642136314236, 0.945080971659919))\n",
        "print(\"Tests passed, but be sure to test your own code as well!\")"
      ],
      "metadata": {
        "id": "4JXNy4VWPryw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Task3: Tuning"
      ],
      "metadata": {
        "id": "-38iECfRPVCd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Tuning `max_depth`\n",
        "\n",
        "Now, we will use the evaluate function to tune the parameter `max_depth`, with the default value for `min_samples_leaf` (which is 1).\n",
        "\n",
        "The code is provided for you, but you will need to interpret the results.\n",
        "\n"
      ],
      "metadata": {
        "id": "9B0x5hnirDXe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_accuracies = []\n",
        "train_accuracies = []\n",
        "\n",
        "for max_depth in range(1,20):\n",
        "  test_acc, train_acc = evaluate(max_depth, 1)\n",
        "  test_accuracies.append(test_acc)\n",
        "  train_accuracies.append(train_acc)\n",
        "\n",
        "  print(\"Max depth\", max_depth)\n",
        "  print(\"\\tTesting Accuracy:\", test_acc)\n",
        "  print(\"\\tTraining Accuracy:\", train_acc)"
      ],
      "metadata": {
        "id": "IHpLX29csYRA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "test = plt.plot(range(1,20), test_accuracies, label = \"Testing\")\n",
        "train = plt.plot(range(1,20), train_accuracies, label = \"Training\")\n",
        "plt.ylim(0, 1)      # to \"zoom in\", you can delete this line\n",
        "plt.xlabel('max_depth')\n",
        "plt.ylabel('accuracy')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "L43ZaIlHtVFt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##‚úè Q4: describe max_depth results.\n",
        "\n",
        "\n",
        "Be sure to discuss overfitting vs. underfitting, as well as which value for `max_depth` you think is best and why."
      ],
      "metadata": {
        "id": "6QR0yIlpGa_Z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "your answer here"
      ],
      "metadata": {
        "id": "6uMi5NT5HNXC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Tuning `min_samples_leaf`\n",
        "\n",
        "Next, we will use the evaluate function to tune the parameter `min_samples_leaf`, with the default value for `max_depth` (which is `None`). The code is provided for you, but you will need to interpret the results."
      ],
      "metadata": {
        "id": "S-86KfVirQTF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_accuracies = []\n",
        "train_accuracies = []\n",
        "\n",
        "for min_samples_leaf in range(1,100):\n",
        "  test_acc, train_acc = evaluate(None, min_samples_leaf)\n",
        "  test_accuracies.append(test_acc)\n",
        "  train_accuracies.append(train_acc)\n",
        "\n",
        "  print(\"Min samples leaf\", min_samples_leaf)\n",
        "  print(\"\\tTesting Accuracy:\", test_acc)\n",
        "  print(\"\\tTraining Accuracy:\", train_acc)"
      ],
      "metadata": {
        "id": "jWsWwHmhtfuu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "test = plt.plot(range(1,100), test_accuracies, label = \"Testing\")\n",
        "train = plt.plot(range(1,100), train_accuracies, label = \"Training\")\n",
        "plt.ylim(0, 1)      # to \"zoom in\", you can delete this line\n",
        "plt.xlabel('min_samples_leaf')\n",
        "plt.ylabel('accuracy')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "QAq6pzSfti2z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##‚úèQ5: Describe min_samples_leaf results\n",
        "\n",
        "Be sure to discuss overfitting vs. underfitting, as well as which value for `min_samples_leaf` you think is best and why."
      ],
      "metadata": {
        "id": "4hBbVPQwIMS6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "your answer here"
      ],
      "metadata": {
        "id": "i145igyQOEu2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##üíªQ6: Best Combination\n",
        "\n",
        "When tuning parameters, it typically takes more effort than just tuning parameters individually. Really, we want the best combination of parameters, so we want to explore them together.\n",
        "\n",
        "One way to do this is with a **grid search**, where we explore a grid of possible parameter values. In the cell below, you will conduct a grid search to find the best combination of `max_depth` and `min_samples_leaf`.\n",
        "\n",
        "*   Using nested `for` loops, use the function `evaluate` to test all combinations of `max_depth` and `min_samples_leaf`, where `max_depth` ranges from 1 through 10, and `min_samples_leaf` ranges from 1 through 50 (can you see how I chose these ranges from the results for the individual parameters above?).\n",
        "*   Remember that the function `evaluate` returns both the testing accuracy and training accuracy. For simplicity, we will just look for the parameters with the **best testing accuracy**.\n",
        "*   For the parameters with the best testing accuracy, store their values in the tuple `parameters` (with `max_depth` first, followed by `min_samples_leaf`). Store their testing accuracy and training accuracy (as found by by evaluate), in the tuple `best_metrics`.\n",
        "\n",
        "The grid search should take a couple of minutes to run. Because it can be time consuming, it is not always possible to run a grid search for big datasets with complex models."
      ],
      "metadata": {
        "id": "4Q8kgwGJraZN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tables.array import Leaf\n",
        "best_metrics = (0,0)\n",
        "parameters = (0,0)\n",
        "\n",
        "# your code here\n",
        "\n",
        "\n",
        "print(\"Best max_depth:\", parameters[0])\n",
        "print(\"Best min_samples_leaf:\", parameters[1])\n",
        "print(\"Testing Accuracy:\", best_metrics[0])\n",
        "print(\"Training Accuracy:\", best_metrics[1])"
      ],
      "metadata": {
        "id": "wUrFn0zNz28w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# TEST CELL - DO NOT CHANGE - RUN THIS CELL TO CHECK YOUR WORK\n",
        "# Note: passing this cell doesn't guarantee your code is correct or that you will get full credit,\n",
        "# but should be used to help you check your work\n",
        "assert(best_metrics == (0.9315013196708584, 0.9578214767688452))\n",
        "print(\"Tests passed, but be sure to test your own code as well!\")"
      ],
      "metadata": {
        "id": "m4uRx73DQNlO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Task 4: Final Model + Reflection\n"
      ],
      "metadata": {
        "id": "BRjHfwPtLOZy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Training a final model\n",
        "\n",
        "Now that we have identified the optimal parameter values, we will train a final model on all of the data, using those parameters values. The idea is that this final model would be used to diagnose tumors.\n",
        "\n",
        "Although we were able to find the optimal parameter values for decision trees, there are other machine learning models that can have even better performance on this dataset. Unfortunately, we don't have time to cover more in this course."
      ],
      "metadata": {
        "id": "UfzZhabjQGOj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.tree import DecisionTreeClassifier\n",
        "\n",
        "clf = DecisionTreeClassifier(max_depth=parameters[0], min_samples_leaf=parameters[1], random_state = 0)\n",
        "clf.fit(X, y)"
      ],
      "metadata": {
        "id": "Cmn5Xlx55Bbw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, we visualize the decision tree that we produced, so that we can see how it makes decisions."
      ],
      "metadata": {
        "id": "4W0xcqSCL2qY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.tree import plot_tree\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.figure(figsize=(20, 10))\n",
        "plot_tree(clf, feature_names = features, class_names = [\"Benign\", \"Malignant\"])\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "MFTSq2Pmz5oz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##‚úèQ7: New record 1\n",
        "\n",
        "Suppose we encounter a new tumor, with values:\n",
        "*   `area_se = 35.021`\n",
        "*   `concave_points_worst = 0.100`\n",
        "*   `concavity_worst = .238`\n",
        "*   `radius_worst = 17.106`\n",
        "*   `texture_mean = 15.987`\n",
        "*   `texture_worst = 26.832`\n",
        "\n",
        "How would the decision tree model classify this tumor? Explain your answer by describing the path this datapoint takes through the tree."
      ],
      "metadata": {
        "id": "_sKL2sspL-FZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "your answer here"
      ],
      "metadata": {
        "id": "DSDRGuXQNOOH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##‚úèQ8: New record 2\n",
        "\n",
        "Suppose we encounter a new tumor, with values:\n",
        "*   `area_se = 39.542`\n",
        "*   `concave_points_worst = 0.154`\n",
        "*   `concavity_worst = .221`\n",
        "*   `radius_worst = 16.234`\n",
        "*   `texture_mean = 16.785`\n",
        "*   `texture_worst = 25.979`\n",
        "\n",
        "How would the decision tree model classify this tumor? Explain your answer by describing the path this datapoint takes through the tree."
      ],
      "metadata": {
        "id": "bfRYkcIdNQeG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "your answer here"
      ],
      "metadata": {
        "id": "7DnzbcAMNRI_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##‚úè Q9: Reflection\n",
        "\n",
        "What did you like about it? What could be improved? Your answers will not affect your overall grade. This feedback will be used to improve future programming assignments."
      ],
      "metadata": {
        "id": "Isom1qdmOiiw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Grading\n",
        "\n",
        "For each of the following accomplishments, there is a breakdown of points which total to 20. The fraction of points earned out of 20 will be multiplied by 5 to get your final score (e.g. 17 points earned will be 17/20 * 5 ‚Üí 4.25)\n",
        "* (1pt) Task1 Q1: Uses correct cross validation\n",
        "* (1pt) Task1 Q1: Trains and fits the data on a decision tree\n",
        "* (1pt) Task1 Q1: Predicts on correct set\n",
        "* (1pt) Task1 Q1: Finds all three evaluation metrics and appends to list\n",
        "* (2pt) Task1 Q2: Correctly interprets accuracy and recall results\n",
        "* (1pt) Task2 Q3: Uses stratified fold correctly\n",
        "* (1pt) Task2 Q3: Finds test and training sets\n",
        "* (1pt) Task2 Q3: Trains and fits a decision tree\n",
        "* (1pt) Task2 Q3: Makes predictions\n",
        "* (1pt) Task3 Q4: Describes max_depth results including over/under fitting and best value\n",
        "* (1pt) Task3 Q5: Describes min_samples_leaf results including over/under fitting and best value\n",
        "* (2pt) Task3 Q6 : Correctly used for loops to loop through hyperparameters\n",
        "* (1pt) Task3 Q6 : Using evaluate function from Task 2\n",
        "* (2pt) Task3 Q6 : Updates best model appropriately\n",
        "* (1pt) Task4 Q7: Correctly classifies new tumor using decision tree path\n",
        "* (1pt) Task4 Q8: Correctly classifies new tumor using decision tree path\n",
        "* (1pt) Task4 Q9: Thoughtfully reflected on the assignment"
      ],
      "metadata": {
        "id": "Y003ChMrdmnA"
      }
    }
  ]
}