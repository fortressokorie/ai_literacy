{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyPBBp8kQAfQQtF9K0QSR4xE",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/chefs-kiss/ML_J2026/blob/main/PA2_Evaluation_with_BreastCancer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Name:\n",
        "\n",
        "Who you worked with:"
      ],
      "metadata": {
        "id": "tUfa1179ztCG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Objectives\n",
        "The goals of this project are to:\n",
        "*   Implement different cross-validation techniques to evaluate model performance\n",
        "* Audit a model to discuss the ethical considerations in model selection and performance evaluation\n",
        "\n",
        "##Overview\n",
        "In this assignment, you will explore the Wisconsin Breast Cancer dataset and focus on key aspects of model evaluation and resampling techniques. You will comment on the complexity of chosen models in a developed workflow, discuss resampling techniques, as well as the implications on various evaluation metrics. You will also be asked to audit the algorithms using the ethical matrix framework discussed in class.\n",
        "\n",
        "##Schedule\n",
        "Here is the suggested schedule for working on this project:\n",
        "*   Over the weekend, read through the project instructions and complete Task 0.\n",
        "*   By Sunday, 2/23, complete Tasks 1-2 of the project, and start Task 3 of the project.\n",
        "*   By Tuesday, 2/25, complete Tasks 3-4 of the project, and start Task 5.\n",
        "*   By Wednesday, 2/26, complete Task 5 and check your solutions against the grading rubric (included at the end of this workbook), and submit your workbook url through moodle.\n",
        "\n",
        "This project is due on Thursday, 2/27, by 11:59pm.\n"
      ],
      "metadata": {
        "id": "ilyDIUgcBnWy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Task 0: Breast Cancer Workflow\n",
        "\n",
        "You will use the Breast Cancer dataset from sklearn.datasets. It contains features of cell nuclei obtained from breast cancer biopsies, and the target variable indicates whether the tumor is malignant or benign.\n"
      ],
      "metadata": {
        "id": "BEtp2ZAY-vBV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_breast_cancer\n",
        "import pandas as pd\n",
        "\n",
        "# Load the dataset\n",
        "data = load_breast_cancer()\n",
        "X = pd.DataFrame(data.data, columns=data.feature_names)\n",
        "y = pd.Series(data.target, name=\"target\")"
      ],
      "metadata": {
        "id": "29IsipZIAhB2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ‚úè Question 1: Describe dataset\n",
        "\n",
        "* Describe the type of data in our dataset.\n",
        "* What is our target?\n",
        "* What does our feature set contain?"
      ],
      "metadata": {
        "id": "DXd4Wijoqy5s"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##‚úè Question 2: The who behind the data\n",
        "To answer the following questions, you may have to search the internet with search like \"wisconsin breast cancer dataset who is in the data\" or something similar\n",
        "* Can you find who curated this dataset?\n",
        "* Include a url to cite this information.\n",
        "* Can you find the demographics of the individuals in the dataset?\n",
        "* In your opinion, why would these types of questions be important to know when dealing with the data?"
      ],
      "metadata": {
        "id": "D9-EemVPrEe7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Task 1: Model Complexity\n"
      ],
      "metadata": {
        "id": "93DEIOk5CsHS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Before jumping into evaluation and cross-validation, we're going to start by performing basic preprocessing and setting up three models for comparison: a null model (also called a baseline model), a basic model, and a complex model.\n",
        "\n",
        "**Null Model**: This will predict the majority class from our target variable.\n",
        "\n",
        "**Basic Model**: This will be a Logistic Regression classifier.\n",
        "\n",
        "**Complex Model**:This will be a Random Forest classifier."
      ],
      "metadata": {
        "id": "7Qg55VigAj3v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.dummy import DummyClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix\n",
        "\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "\n",
        "null_model = DummyClassifier(strategy='most_frequent', random_state=42)\n",
        "basic_model = LogisticRegression(solver='liblinear', random_state=42)\n",
        "complex_model = RandomForestClassifier(random_state=42)\n",
        "\n",
        "\n",
        "null_model.fit(X_train, y_train)\n",
        "basic_model.fit(X_train, y_train)\n",
        "complex_model.fit(X_train, y_train)\n",
        "\n",
        "\n",
        "null_pred = null_model.predict(X_test)\n",
        "basic_pred = basic_model.predict(X_test)\n",
        "complex_pred = complex_model.predict(X_test)\n",
        "\n",
        "\n",
        "print(\"Null Model Accuracy:\", accuracy_score(y_test, null_pred))\n",
        "print(\"Basic Model Accuracy:\", accuracy_score(y_test, basic_pred))\n",
        "print(\"Complex Model Accuracy:\", accuracy_score(y_test, complex_pred))\n",
        "\n",
        "\n",
        "print(\"Null Model Confusion Matrix:\", confusion_matrix(y_test, null_pred))\n",
        "print(\"Basic Model Confusion Matrix:\", confusion_matrix(y_test, basic_pred))\n",
        "print(\"Complex Model Confusion Matrix:\", confusion_matrix(y_test, complex_pred))"
      ],
      "metadata": {
        "id": "XWaEUlwOAbX9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##üíª Question 3: Comments\n",
        "\n",
        "* Add comments to the code above that describe what each section of code is doing (to the best of your ability). You may want to consult our workbook on cross-validation (EVL2)."
      ],
      "metadata": {
        "id": "-GIgK5eep-e-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ‚úè Question 4: Accuracy Discussion\n",
        "\n",
        "* Compare the accuracy metric for each of the three models. Does increasing model complexity drastically change the accuracy of the models? How well does the null (baseline) model compare to the simple and complex?\n"
      ],
      "metadata": {
        "id": "w3jfJ25nqBlS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Task 2: Resampling\n",
        "\n",
        "For this task, we're going to use different cross-validation techniques to evaluate the models' performance more robustly.\n",
        "\n",
        "* Stratified K-Fold Cross-Validation (to maintain class distribution) using scikit-learn `StratifiedKFold`\n",
        "* Repeated Cross-Validation (to get more robust performance metrics) using scikit-learn `RepeatedStratifiedKFold`\n",
        "* Bootstrapping (random sampling with replacement) using scikit-learn `resample`"
      ],
      "metadata": {
        "id": "aLRlHsCg_Dw5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.utils import resample\n",
        "from sklearn.model_selection import cross_val_score, StratifiedKFold, RepeatedStratifiedKFold\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "qC8vaXnMdIZ_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Stratified K-Fold Cross-Validation\n",
        "stratified_cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "stratified_scores = cross_val_score(basic_model, X_train, y_train, cv=stratified_cv)\n",
        "print(\"Stratified K-Fold Cross-Validation Scores:\", stratified_scores)\n",
        "print(\"Mean Accuracy:\", np.mean(stratified_scores))"
      ],
      "metadata": {
        "id": "1z6ONlGOdEsS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Repeated Cross-Validation\n",
        "repeated_cv = RepeatedStratifiedKFold(n_splits=5, n_repeats=3, random_state=42)\n",
        "repeated_scores = cross_val_score(basic_model, X_train, y_train, cv=repeated_cv)\n",
        "print(\"Repeated Cross-Validation Scores:\", repeated_scores)\n",
        "print(\"Mean Accuracy:\", np.mean(repeated_scores))"
      ],
      "metadata": {
        "id": "-tg0IFqVdKJ1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Bootstrapping (Resampling)\n",
        "bootstrap_scores = []\n",
        "for _ in range(50):\n",
        "    X_resampled, y_resampled = resample(X_train, y_train, random_state=42)\n",
        "    basic_model.fit(X_resampled, y_resampled)\n",
        "    score = basic_model.score(X_test, y_test)\n",
        "    bootstrap_scores.append(score)\n",
        "\n",
        "print(\"Bootstrapping Accuracy (100 resamples):\", np.mean(bootstrap_scores))"
      ],
      "metadata": {
        "id": "vFK9PLf3CpEs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##üíª Question 5: Comments\n",
        "This is similar to #3\n",
        "* Add comments to each of the code chunks above."
      ],
      "metadata": {
        "id": "ktb9kht7qUB2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##‚úè Question 6: Accuracy Discussion\n",
        "This is the same question as #4, but now considering the resampled accuracy metrics.\n",
        "* Describe how to compare the accuracy metric for each of the three models.\n"
      ],
      "metadata": {
        "id": "pusZqjX0qXnT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ‚úè Question 7: Continued evaluation\n",
        "\n",
        "Even if a model has high accuracy, it may not be the best choice for our given situation.\n",
        "\n",
        "* Describe why this may be.\n",
        "* What would be a solution for this problem?"
      ],
      "metadata": {
        "id": "_B9jpje0qkRx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Task 3: Evaluation Metrics\n",
        "\n",
        "In this task, we will evaluate the performance of the models using different evaluation metrics such as accuracy, precision, recall, and F1 score."
      ],
      "metadata": {
        "id": "GIKjCkqb_GWr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import precision_score, recall_score, f1_score\n",
        "\n",
        "# Train and evaluate model\n",
        "models = [(null_model, \"null model\"), (basic_model, \"basic model\"), (complex_model, \"complex model\")]\n",
        "for model_type in models:\n",
        "  y_pred = model_type[0].predict(X_test)\n",
        "  # Evaluate\n",
        "  print(f\"{model_type[1]}: {model_type[0]}\")\n",
        "  print(\"Accuracy:\", round(accuracy_score(y_test, y_pred),3))\n",
        "  print(\"Precision:\", round(precision_score(y_test, y_pred),3))\n",
        "  print(\"Recall:\", round(recall_score(y_test, y_pred),3))\n",
        "  print(\"F1 Score:\", round(f1_score(y_test, y_pred),3), \"\\n\")\n"
      ],
      "metadata": {
        "id": "IkXguSL8CyPT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Questions 8-11: Interpret Metrics\n",
        "\n",
        "For each model, interpret the following evaluation metrics:"
      ],
      "metadata": {
        "id": "3OIfbnQKEQ0L"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##‚úè Question 8: Accuracy\n",
        "\n",
        "Null model:\n",
        "\n",
        "Basic model:\n",
        "\n",
        "Complex model:\n"
      ],
      "metadata": {
        "id": "fj30FHa8EVB5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##‚úè Question 9: Precision\n",
        "\n",
        "Null model:\n",
        "\n",
        "Basic model:\n",
        "\n",
        "Complex model:"
      ],
      "metadata": {
        "id": "33nKuW6DEbtC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##‚úè Question 10: Recall\n",
        "\n",
        "Null model:\n",
        "\n",
        "Basic model:\n",
        "\n",
        "Complex model:"
      ],
      "metadata": {
        "id": "wuhTMiHxEeBT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##‚úè Question 11: Which is the best\n",
        "Once you have interpreted all of metrics, we‚Äôd like to choose which model is the best given the problem. You‚Äôll need to consider the trade-offs between precision, recall, and accuracy and how that impacts the model‚Äôs suitability for real-world application, especially in a healthcare context.\n",
        "\n",
        "* Discuss which of the models is the best performing."
      ],
      "metadata": {
        "id": "WJ0LeqFTEjyk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Task 4: Ethical Matrix and Audit\n",
        "\n",
        "In this task, you will create an ethical matrix to audit the algorithms you have been working with. The goal is to analyze the broader implications of your machine learning model and how it might affect different stakeholders. Following the guidelines outlined in class, your ethical audit should include key questions that explore the potential harms, benefits, fairness, and accountability of the algorithm."
      ],
      "metadata": {
        "id": "1iCvDh0B_JhZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##‚úè Question 12: Define the key stakeholders\n",
        "\n",
        "In any algorithmic system, there are various stakeholders who will be affected by the decisions made by the model. For example, one stakeholder is the patient being screened for breast cancer.\n",
        "\n",
        "* Add at least three more stakeholders:"
      ],
      "metadata": {
        "id": "YVRgOQLi_O9F"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Questions 13-15: Identify the ethical dimensions\n",
        "\n",
        "For each stakeholder, consider the potential benefits and harms of using the model. Also, think about the ethical principles of fairness and accountability.\n",
        "\n",
        "The main issues we've covered in class are listed below:\n",
        "* **Benefits**: What positive outcomes might each stakeholder experience if the model is deployed?\n",
        "* **Harms**: What potential negative consequences might arise for each stakeholder?\n",
        "Could the model cause harm or lead to incorrect conclusions?\n",
        "* **Fairness**: Is the model equally fair to all stakeholders, especially those from under-represented or vulnerable groups? Does the model avoid reinforcing bias?\n",
        "* **Accountability**: Who is responsible if the model makes a mistake? What steps should be taken if the model‚Äôs predictions are inaccurate or harmful?\n",
        "\n",
        "If you decide to consider different issues, make sure to update the text boxes below to account for that. For example, if you want to consider something like medical advances you might replace the issue `fairness` with `medical advances`."
      ],
      "metadata": {
        "id": "k1n_1yQ5_dBV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##‚úè Question 13:\n",
        "\n",
        "Stakeholder1:\n",
        "\n",
        "Benefits:\n",
        "\n",
        "Harms:\n",
        "\n",
        "Fairness:\n",
        "\n",
        "Accountability:\n",
        "\n"
      ],
      "metadata": {
        "id": "QJpd-Xef_mac"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##‚úè Question 14:\n",
        "\n",
        "Stakeholder2:\n",
        "\n",
        "Benefits:\n",
        "\n",
        "Harms:\n",
        "\n",
        "Fairness:\n",
        "\n",
        "Accountability:\n",
        "\n"
      ],
      "metadata": {
        "id": "TF8MD3yK_ohY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##‚úè Question 15:\n",
        "\n",
        "Stakeholder3:\n",
        "\n",
        "Benefits:\n",
        "\n",
        "Harms:\n",
        "\n",
        "Fairness:\n",
        "\n",
        "Accountability:\n",
        "\n"
      ],
      "metadata": {
        "id": "7C_Bx5NZ_zI9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##‚úè Question 16: Mitigating Harm\n",
        "\n",
        "Based on your matrix, come up with two concrete actions that can be taken to mitigate potential harms.\n",
        "Example action: we‚Äôd like to ensure the model is interpretable for healthcare providers so they can understand and trust its predictions. This means we‚Äôd need to choose a model that is less complex but still robust enough to give good results.\n",
        "\n",
        "* First option:\n",
        "\n",
        "\n",
        "* Second option:"
      ],
      "metadata": {
        "id": "ltRope3x_hZo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task 5: Reflection\n",
        "\n",
        "Take a moment to reflect on this assignment."
      ],
      "metadata": {
        "id": "AJgHjLb_T7WX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##‚úè Question 17:\n",
        "\n",
        "What did you like about it? What could be improved? Your answers will not affect your overall grade. This feedback will be used to improve future programming assignments.\n",
        "\n"
      ],
      "metadata": {
        "id": "ZFK6Nqs2UPAJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Submission\n",
        "\n",
        "You will be submitting your code using Moodle. For this project, you will need to submit the url to your colab workbook. Make sure you have shared access to your notebook, and create a link as your submission."
      ],
      "metadata": {
        "id": "VR4tBSzVxeq8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Grading\n",
        "For each of the following accomplishments, there is a breakdown of points which total to 20. The fraction of points earned out of 20 will be multiplied by 5 to get your final score (e.g. 17 points earned will be 17/20 * 5 ‚Üí 4.25)\n",
        "* (1pt) Task0 q1: You have described the information in the dataset, and identified target and feature sets.\n",
        "* (1pt) Task0 q2: You have sourced information about the dataset\n",
        "* (1pt) Task0 q2: You have considered why this would be important.\n",
        "* (2pt) Task1 q3 and Task2 q5: You have added informative comments to the code\n",
        "* (1pt) Task1 q4 and Task2 q6: You have discussed an alternative to accuracy.\n",
        "* (1pt) Task2 q7: Two reasonable options have been provided.\n",
        "* (1pt) Task3 q8: You have correctly interpreted all three accuracy metrics.\n",
        "* (1pt) Task3 q9: You have correctly interpreted all three precision metrics.\n",
        "* (1pt) Task3 q10: You have correctly interpreted all three recall metrics.\n",
        "* (1pt) Task3 q11: You have correctly identified the best model out of the null, basic, and complex models.\n",
        "* (1pt) Task4 q12: You‚Äôve identified at least three stakeholders.\n",
        "* (3pt) Task4 q13-15: You‚Äôve filled out the rest of the ethical matrix.\n",
        "* (1pt) Task4 q13-15: Your ethical matrix is thoughtful.\n",
        "* (1pt) Task4 q16: You have identified two ways to mitigate harm.\n",
        "* (1pt) Task4 q16: Your solutions to mitigate harm are thoughtful.\n",
        "* (1pt) Task5 q17: You have reflected on the assignment.\n",
        "* (1pt) Task5 q17: Your reflection is thoughtful.\n"
      ],
      "metadata": {
        "id": "m9OFgROxxgtx"
      }
    }
  ]
}